{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "150326cf",
      "metadata": {
        "id": "150326cf"
      },
      "source": [
        "## Лабораторная работа 1. Морфологический парсер mystem"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9f96232",
      "metadata": {
        "id": "d9f96232"
      },
      "source": [
        "**Задание 1.** Изучите документацию и лицензию (!) морфологического парсера mystem от Yandex: https://yandex.ru/dev/mystem."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fccc662",
      "metadata": {
        "id": "8fccc662"
      },
      "source": [
        "**Задание 2.** Установите `pymystem3` – интерфейс к mystem на Python: https://pypi.org/project/pymystem3.\n",
        "\n",
        "(!) Обратите внимание, что у конструктора объекта Mystem() есть параметры."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Устанавливаем pymystem3\n",
        "!pip install pymystem3\n",
        "from pymystem3 import Mystem\n",
        "import json"
      ],
      "metadata": {
        "id": "_S0maWHIrKrQ"
      },
      "id": "_S0maWHIrKrQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ieNFd9YCR5-q"
      },
      "id": "ieNFd9YCR5-q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = Mystem()"
      ],
      "metadata": {
        "id": "4CveZRWkri4-"
      },
      "id": "4CveZRWkri4-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "afc49582",
      "metadata": {
        "id": "afc49582"
      },
      "source": [
        "**Задание 3.** Выпишите с какими параметрами запускается морфологический анализатор.\n",
        "\n",
        "При создании объекта Mystem() в pymystem3 можно передать следующие ключевые параметры (флаги, соответствующие флагам бинарного mystem):\n",
        "\n",
        "-i (disambiguation=True/False): Включение или отключение контекстной омонимии. По умолчанию True (включено). Анализатор выбирает наиболее подходящий разбор слова в данном контексте.\n",
        "\n",
        "-d (entire_input=False/True): Отключение или включение разрешения омонимии для всего текста целиком. По умолчанию False (работает по предложениям). При установке True анализатор будет использовать для снятия омонимии контекст всего переданного текста, а не отдельного предложения.\n",
        "\n",
        "-format (format=\"text\" / \"json\"): Формат вывода. По умолчанию json. Можно задать text для вывода в текстовом виде (редко используется через pymystem3, так как json гораздо удобнее для программной обработки).\n",
        "\n",
        "-weight (weight=True/False): Выдача информации о весе (частоте) разбора. По умолчанию False.\n",
        "\n",
        "\n",
        "**ЧР: A=им** означает:\n",
        "\n",
        "- **ЧР** — Часть Речи\n",
        "- **A** — прилагательное (от лат. \"Adjectivum\")\n",
        "- **=им** — в именительном падеже\n",
        "\n",
        "## Расшифровка основных обозначений частей речи в mystem:\n",
        "\n",
        "| Сокращение | Часть речи          | Пример        |\n",
        "|------------|---------------------|---------------|\n",
        "| **A**      | Прилагательное      | красивый      |\n",
        "| **S**      | Существительное     | стол          |\n",
        "| **V**      | Глагол              | делать        |\n",
        "| **ADV**    | Наречие             | быстро        |\n",
        "| **PR**     | Предлог             | в, на, под    |\n",
        "| **CONJ**   | Союз                | и, но, чтобы  |\n",
        "| **PART**   | Частица             | бы, же, ли    |\n",
        "| **SPRO**   | Местоимение-существительное | я, ты, он |\n",
        "| **APRO**   | Местоименное прилагательное | мой, твой |\n",
        "| **NUM**    | Числительное        | пять, второй  |\n",
        "| **INTJ**   | Междометие          | ой, ах        |\n",
        "\n",
        "## Грамматические характеристики (после запятой):\n",
        "\n",
        "- **падежи**: им (именительный), рд (родительный), дт (дательный) и т.д.\n",
        "- **число**: ед (единственное), мн (множественное)\n",
        "- **род**: мр (мужской), жр (женский), ср (средний)\n",
        "- **время**: прош (прошедшее), наст (настоящее), буд (будущее)\n",
        "\n",
        "## Примеры разбора:\n",
        "\n",
        "- **ЧР: S,жен,неод=им,ед** — существительное, женский род, неодушевленное, именительный падеж, единственное число\n",
        "- **ЧР: V,несов=прош,ед,изъяв,мр** — глагол, несовершенный вид, прошедшее время, единственное число, изъявительное наклонение, мужской род\n",
        "- **ЧР: A=им,ед,мр** — прилагательное, именительный падеж, единственное число, мужской род\n",
        "\n",
        "Так что **A=им** — это прилагательное в именительном падеже.\n",
        "\n",
        "**а)** Придумайте и запишите примеры предложений со словами не из словаря. Приведите их полученные морфологические разборы.\n",
        "\n",
        "### 1. Ищем танка, хила и дамагера для пати на данж.\n",
        "\n",
        "**Слова не из словаря (игровой сленг):**\n",
        "*   **Танка** (сущ.) — от англ. `tank` (танк, броня). В играх: персонаж, который принимает на себя урон врагов.\n",
        "    *   **Начальная форма:** танк (в данном контексте используется в разговорной форме с суффиксом -а).\n",
        "    *   **Признаки:** нариц., одуш., м.р., 1-го склонения. Употреблено в **винительном падеже**, ед. число. В предложении является **дополнением** (ищем (кого?) танка).\n",
        "*   **Хила** (сущ.) — от англ. `heal` (лечение). В играх: персонаж, который лечит других.\n",
        "    *   **Начальная форма:** хил (или хилер).\n",
        "    *   **Признаки:** нариц., одуш., м.р., 1-го склонения. Употреблено в **винительном падеже**, ед. число. В предложении является **дополнением** (ищем (кого?) хила).\n",
        "*   **Дамагера** (сущ.) — от англ. `damage` (урон). В играх: персонаж, наносящий большой урон.\n",
        "    *   **Начальная форма:** дамагер.\n",
        "    *   **Признаки:** нариц., одуш., м.р., 2-го склонения. Употреблено в **винительном падеже**, ед. число (окончание -а вместо -я). В предложении является **дополнением** (ищем (кого?) дамагера).\n",
        "*   **Пати** (сущ.) — от англ. `party` (вечеринка, группа). В играх: группа игроков.\n",
        "    *   **Начальная форма:** пати (нескл.).\n",
        "    *   **Признаки:** нариц., неодуш., ср.р., неизм. слово. Употреблено в **винительном падеже**. В предложении является **дополнением** (для (чего?) пати).\n",
        "*   **Данж** (сущ.) — от англ. `dungeon` (подземелье). В играх: локация для прохождения командой.\n",
        "    *   **Начальная форма:** данж.\n",
        "    *   **Признаки:** нариц., неодуш., м.р., 2-го склонения. Употреблено в **винительном падеже** с предлогом \"на\". В предложении является **обстоятельством места** (на (что?) данж).\n",
        "\n",
        "**Общий разбор:** Предложение повествовательное, простое. Грамматическая основа: **Мы (подлежащее, опущено) ищем (сказуемое)**. Остальные слова являются дополнениями и обстоятельством.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Я купил вчера куловые диски и проапререйлил свой мустанг.\n",
        "\n",
        "**Слова не из словаря (автомобильный тюнинг):**\n",
        "*   **Куловые** (прил.) — от англ. `cool` (крутой, охлаждающий). Разг. вариант от \"охлаждающие\".\n",
        "    *   **Начальная форма:** куловый (в общелит. языке — \"охлаждающий\").\n",
        "    *   **Признаки:** относительное прилагательное. Употреблено во **множественном числе**, **винительном падеже**. В предложении является **определением** (диски (какие?) куловые).\n",
        "*   **Проапререйлил** (глаг.) — от англ. `upgrade` (улучшать, модернизировать). Жаргонная форма, прошедшая через разговорную трансформацию (upgrade -> апгрейдить -> проапрейдить).\n",
        "    *   **Начальная форма:** проапререйлить (в общелит. языке — \"апгрейдить\", \"модернизировать\").\n",
        "    *   **Признаки:** глагол совершенного вида, переходный, невозвратный, 1-го спряжения. Употреблен в **изъявительном наклонении**, **прошедшем времени**, **мужском роде**, **единственном числе**. В предложении является **однородным сказуемым** (купил и проапререйлил).\n",
        "*   **Мустанг** (сущ.) — здесь: марка автомобиля Ford Mustang. Хотя слово является именем собственным, в данном контексте оно используется как нарицательное для обозначения конкретной машины говорящего.\n",
        "    *   **Начальная форма:** Мустанг.\n",
        "    *   **Признаки:** собственное, неодуш., м.р., 2-го склонения. Употреблено в **винительном падеже**, ед. число. В предложении является **дополнением** (проапререйлил (что?) свой мустанг).\n",
        "\n",
        "**Общий разбор:** Предложение повествовательное, простое, с однородными сказуемыми. Грамматическая основа: **Я (подлежащее) купил и проапререйлил (однородные сказуемые)**.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Нужно подготовить нормативку на подпись, потом заняться текучкой.\n",
        "\n",
        "**Слова не из словаря (офисный, корпоративный жаргон):**\n",
        "*   **Нормативку** (сущ.) — сокращение от \"нормативная документация\" (документы по стандартам, правилам, ГОСТам).\n",
        "    *   **Начальная форма:** нормативка.\n",
        "    *   **Признаки:** нариц., неодуш., ж.р., 1-го склонения. Употреблено в **винительном падеже**, ед. число. В предложении является **дополнением** (подготовить (что?) нормативку).\n",
        "*   **Текучкой** (сущ.) — от прил. \"текущий\". Обозначает небольшие, рутинные, ежедневные дела, постоянную работу.\n",
        "    *   **Начальная форма:** текучка.\n",
        "    *   **Признаки:** нариц., неодуш., ж.р., 1-го склонения. Употреблено в **творительном падеже** с предлогом \"заняться\". В предложении является **дополнением** (заняться (чем?) текучкой).\n",
        "\n",
        "**Общий разбор:** Предложение повествовательное, простое, безличное. Грамматическая основа: **Нужно подготовить и заняться (составное глагольное сказуемое в безличной конструкции)**. Подлежащего нет.\n",
        "\n",
        "**б)** Применяется ли контекстное снятие омонимии при морфологическом разборе?\n",
        "\n",
        "Да, применяется, и это одна из ключевых функций mystem.\n",
        "\n",
        "Контекстное снятие омонимии активно используется по умолчанию (когда параметр disambiguation=True). Алгоритм анализирует окружение слова (соседние слова, их грамматические характеристики, синтаксические связи) чтобы выбрать наиболее вероятную лемму и набор грамматических тегов из всех возможных."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Пример анализа текста\n",
        "text = \"Мама мыла раму.\"\n",
        "analysis = m.analyze(text)\n",
        "print(analysis)"
      ],
      "metadata": {
        "id": "1iJjI7N7rtuV"
      },
      "id": "1iJjI7N7rtuV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "fa18ab99",
      "metadata": {
        "id": "fa18ab99"
      },
      "source": [
        "**Задание 4.** Напишите функцию `parse_text()`, на вход которой поступает текст (в виде строки), а на выходе формируется структура данных, содержащая для каждого слова входного текста следующую информацию:\n",
        "- исходную словоформу (wordform);\n",
        "- нормальную форму слова (лемму) (norm, lemma);\n",
        "- часть речи (part of speech, POS);\n",
        "- другую грамматическую информацию, выдаваемую mystem;\n",
        "- признак, присутствует ли слово в словаре mystem.\n",
        "\n",
        "Функция должна выбирать наиболее вероятный вариант морфологического разбора слова."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fa032ea",
      "metadata": {
        "id": "0fa032ea"
      },
      "outputs": [],
      "source": [
        "def parse_text(text):\n",
        "    \"\"\"\n",
        "    Производит морфологический разбор текста с помощью mystem.\n",
        "    \"\"\"\n",
        "    m = Mystem(disambiguation=True)\n",
        "    analysis_result = m.analyze(text)\n",
        "    parsed_data = []\n",
        "\n",
        "    for item in analysis_result:\n",
        "        # Пропускаем пробелы, знаки препинания и пустые элементы\n",
        "        if not item.get('analysis') or not item['text'].strip():\n",
        "            continue\n",
        "\n",
        "        parsed = item['analysis'][0]\n",
        "        gram_info = parsed.get('gr', '')\n",
        "        pos = gram_info.split('=')[0].split(',')[0] if gram_info else 'UNKN'\n",
        "        lemma = parsed.get('lex', item['text'].lower())\n",
        "\n",
        "        # Определение наличия в словаре\n",
        "        qual = parsed.get('qual', '')\n",
        "        in_dict = qual != 'bastard'  # Если не помечено как 'bastard', то в словаре\n",
        "\n",
        "        word_info = {\n",
        "            'wordform': item['text'],\n",
        "            'lemma': lemma,\n",
        "            'pos': pos,\n",
        "            'gram_info': gram_info,\n",
        "            'in_dict': in_dict\n",
        "        }\n",
        "\n",
        "        parsed_data.append(word_info)\n",
        "\n",
        "    return parsed_data\n",
        "\n",
        "# Пример использования функции\n",
        "test_text = \"красивая мама красиво мыла раму.раврарап\"\n",
        "result = parse_text(test_text)\n",
        "\n",
        "print(\"Результат морфологического разбора:\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "for i, word_info in enumerate(result, 1):\n",
        "    print(f\"{i:2d}. Слово: '{word_info['wordform']:15}' \"\n",
        "          f\"Лемма: '{word_info['lemma']:15}' \"\n",
        "          f\"ЧР: {word_info['pos']:5} \"\n",
        "          f\"В словаре: {word_info['in_dict']!s:5} \"\n",
        "          f\"Грам.инфо: {word_info['gram_info']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45237dc3",
      "metadata": {
        "id": "45237dc3"
      },
      "source": [
        "**Задание 5.** Напишите функцию `save_morph_results()`, сохраняющую структуру данных, получаемую функцией `parse_text()`, в текстовый файл формата JSON."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "912ddbe9",
      "metadata": {
        "id": "912ddbe9"
      },
      "outputs": [],
      "source": [
        "# Функция для сохранения результатов в JSON\n",
        "def save_morph_results(data, filename=\"morph_results.json\"):\n",
        "    \"\"\"\n",
        "    Сохраняет результаты разбора в JSON файл\n",
        "    \"\"\"\n",
        "    # Создаем структуру для сохранения\n",
        "    result_to_save = {\n",
        "        'total_words': len(data),\n",
        "        'words': data\n",
        "    }\n",
        "\n",
        "    # Сохраняем в файл\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        json.dump(result_to_save, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\" Результаты сохранены в файл: {filename}\")\n",
        "    print(f\" Обработано слов: {len(data)}\")\n",
        "\n",
        "# Текст для анализа\n",
        "text = \"Мама мыла раму. Она приготовила ужин.\"\n",
        "\n",
        "print(\" Анализируем текст:\", text)\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Разбираем текст\n",
        "parsed_data = parse_text(text)\n",
        "\n",
        "# Показываем результаты\n",
        "print(\" Результаты разбора:\")\n",
        "for i, word in enumerate(parsed_data, 1):\n",
        "    dict_status = \" В словаре\" if word['in_dict'] else \"Не в словаре\"\n",
        "    print(f\"{i:2d}. {word['wordform']:12} -> {word['lemma']:12} ({word['pos']:4}) {dict_status}\")\n",
        "\n",
        "# Сохраняем в файл\n",
        "save_morph_results(parsed_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "621548a6",
      "metadata": {
        "id": "621548a6"
      },
      "source": [
        "**Задание 6.** Напишите функцию `get_dictionary()`, на вход которой поступает текст (в виде строки), а на выходе формируется словарь,\n",
        "включающий все уникальные слова текста и содержащий для каждого слова следующую информацию:\n",
        "- нормальную форму слова;\n",
        "- часть речи;\n",
        "- частоту слова в тексте;\n",
        "- все варианты словоформ в тексте с данной нормальной формой."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf5fdae8",
      "metadata": {
        "id": "bf5fdae8"
      },
      "outputs": [],
      "source": [
        "def get_dictionary(text):\n",
        "    \"\"\"\n",
        "    Создает словарь уникальных слов текста с информацией о них.\n",
        "    \"\"\"\n",
        "\n",
        "    m = Mystem(disambiguation=True)\n",
        "    analysis_result = m.analyze(text)\n",
        "    result = {}\n",
        "\n",
        "    for item in analysis_result:\n",
        "        # Пропускаем пробелы и пустые элементы\n",
        "        if not item.get('analysis') or not item['text'].strip():\n",
        "            continue\n",
        "\n",
        "        parsed = item['analysis'][0]\n",
        "        lemma = parsed.get('lex', item['text'].lower()).lower()\n",
        "        pos = parsed.get('gr', '').split(',')[0].split('=')[0] if parsed.get('gr') else 'UNKN'\n",
        "        wordform = item['text']\n",
        "\n",
        "        if lemma not in result:\n",
        "            result[lemma] = {\n",
        "                'pos': pos,\n",
        "                'frequency': 1,\n",
        "                'wordforms': {wordform}  # Используем множество для уникальности\n",
        "            }\n",
        "        else:\n",
        "            result[lemma]['frequency'] += 1\n",
        "            result[lemma]['wordforms'].add(wordform)\n",
        "\n",
        "    # Преобразуем множества обратно в списки\n",
        "    for lemma in result:\n",
        "        result[lemma]['wordforms'] = list(result[lemma]['wordforms'])\n",
        "\n",
        "    return result\n",
        "\n",
        "# Пример текста для анализа\n",
        "text = \"Красивые кошки любят бегать быстро. Кошки любят играть. Красивая кошка спит.\"\n",
        "\n",
        "# Вызов функции\n",
        "result_dict = get_dictionary(text)\n",
        "\n",
        "# Вывод результатов\n",
        "for lemma, data in result_dict.items():\n",
        "    print(f\"Лемма: {lemma}\")\n",
        "    print(f\"Часть речи: {data['pos']}\")\n",
        "    print(f\"Частота: {data['frequency']}\")\n",
        "    print(f\"Словоформы: {data['wordforms']}\")\n",
        "    print(\"-\" * 30)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4eeb0797",
      "metadata": {
        "id": "4eeb0797"
      },
      "source": [
        "**Задание 7.** Напишите функцию `save_dictionary()`, сохраняющую предыдущую структуру данных в текстовый файл формата JSON. Слова в файле должны быть упорядочены по убыванию частоты."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10a36600",
      "metadata": {
        "id": "10a36600"
      },
      "outputs": [],
      "source": [
        "def save_dictionary(dictionary, filename):\n",
        "    \"\"\"\n",
        "    Сохраняет словарь в JSON файл с упорядочиванием по убыванию частоты.\n",
        "\n",
        "    Args:\n",
        "        dictionary (dict): Словарь, полученный из get_dictionary()\n",
        "        filename (str): Имя файла для сохранения\n",
        "    \"\"\"\n",
        "    # Сортируем словарь по убыванию частоты\n",
        "    sorted_items = sorted(\n",
        "        dictionary.items(),\n",
        "        key=lambda x: x[1]['frequency'],\n",
        "        reverse=True\n",
        "    )\n",
        "\n",
        "    # Создаем отсортированный словарь\n",
        "    sorted_dict = {item[0]: item[1] for item in sorted_items}\n",
        "\n",
        "    # Сохраняем в файл\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        json.dump(sorted_dict, f, ensure_ascii=False, indent=2, sort_keys=False)\n",
        "\n",
        "# Пример текста для анализа\n",
        "text = \"Красивые кошки любят бегать быстро. Кошки любят играть. Красивая кошка спит.\"\n",
        "\n",
        "# Создаем словарь\n",
        "result_dict = get_dictionary(text)\n",
        "\n",
        "# Сохраняем в файл\n",
        "save_dictionary(result_dict, \"dictionary.json\")\n",
        "\n",
        "# Выводим результаты для проверки\n",
        "print(\"Содержимое словаря:\")\n",
        "for lemma, data in result_dict.items():\n",
        "    print(f\"Лемма: {lemma}\")\n",
        "    print(f\"Часть речи: {data['pos']}\")\n",
        "    print(f\"Частота: {data['frequency']}\")\n",
        "    print(f\"Словоформы: {data['wordforms']}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "print(\"Словарь сохранен в файл 'dictionary.json'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b86bdd0c",
      "metadata": {
        "id": "b86bdd0c"
      },
      "source": [
        "**Задание 8.** Напишите функцию `get_non_mystem_dict()`, на вход которой поступает структура данных, получаемая функцией `parse_text()`, а на выходе формируется словарь, содержащий уникальные слова текста, отсутствующие в словаре mystem, вместе с частотой слова в тексте. **Остановились на этом**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7299673",
      "metadata": {
        "id": "a7299673"
      },
      "outputs": [],
      "source": [
        "def get_non_mystem_dict(parsed_data):\n",
        "    \"\"\"\n",
        "    Создает словарь слов, отсутствующих в словаре mystem, с их частотой.\n",
        "\n",
        "    Args:\n",
        "        parsed_data (list): Структура данных от parse_text()\n",
        "\n",
        "    Returns:\n",
        "        dict: Словарь, где ключ - словоформа, значение - частота в тексте\n",
        "    \"\"\"\n",
        "    non_dict_words = {}\n",
        "\n",
        "    for word_info in parsed_data:\n",
        "        if not word_info['in_dict']:\n",
        "            wordform = word_info['wordform']\n",
        "            if wordform in non_dict_words:\n",
        "                non_dict_words[wordform] += 1\n",
        "            else:\n",
        "                non_dict_words[wordform] = 1\n",
        "\n",
        "    return non_dict_words\n",
        "\n",
        "# Тестирование функции\n",
        "test_text = \"красивая мама красиво мыла раму. Дамагера засумонили в данж\"\n",
        "\n",
        "# Разбираем текст\n",
        "parsed_result = parse_text(test_text)\n",
        "\n",
        "# Получаем словарь слов не из mystem\n",
        "non_mystem_words = get_non_mystem_dict(parsed_result)\n",
        "\n",
        "print(\"Слова, отсутствующие в словаре mystem:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for word, frequency in non_mystem_words.items():\n",
        "    print(f\"'{word}': {frequency}\")\n",
        "\n",
        "print(f\"\\nВсего уникальных слов не из словаря: {len(non_mystem_words)}\")\n",
        "print(f\"Общее количество таких слов в тексте: {sum(non_mystem_words.values())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d3d5fc1",
      "metadata": {
        "id": "9d3d5fc1"
      },
      "source": [
        "**Задание 9.** Напишите функцию `save_non_mystem_dict()`, сохраняющую структуру данных, получаемую функцией `get_non_mystem_dict()`, в текстовый файл формата TSV (tab-separated values). Слова в файле должны быть упорядочены по убыванию частоты."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e29ebb1",
      "metadata": {
        "id": "9e29ebb1"
      },
      "outputs": [],
      "source": [
        "def save_non_mystem_dict(non_dict_words, filename):\n",
        "    \"\"\"\n",
        "    Сохраняет словарь слов, отсутствующих в mystem, в TSV файл.\n",
        "    Слова упорядочены по убыванию частоты.\n",
        "\n",
        "    Args:\n",
        "        non_dict_words (dict): Словарь от get_non_mystem_dict()\n",
        "        filename (str): Имя файла для сохранения\n",
        "    \"\"\"\n",
        "    # Сортируем словарь по убыванию частоты\n",
        "    sorted_words = sorted(\n",
        "        non_dict_words.items(),\n",
        "        key=lambda x: x[1],\n",
        "        reverse=True\n",
        "    )\n",
        "\n",
        "    # Сохраняем в TSV файл\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        # Записываем заголовок\n",
        "        f.write(\"Слово\\tЧастота\\n\")\n",
        "\n",
        "        # Записываем данные\n",
        "        for word, frequency in sorted_words:\n",
        "            f.write(f\"{word}\\t{frequency}\\n\")\n",
        "\n",
        "# Сохраняем в файл\n",
        "filename = \"non_mystem_words.tsv\"\n",
        "save_non_mystem_dict(non_mystem_words, filename)\n",
        "\n",
        "print(f\"\\nФайл '{filename}' успешно сохранен!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6945245",
      "metadata": {
        "id": "f6945245"
      },
      "source": [
        "**Задание 10.** Напишите функцию `get_pos_distribution()`, на вход которой поступает словарь, формируемый функцией `get_dictionary()`, а на выходе выдается структура данных, содержащая частотное распределение частей речи в словаре со следующими значениями\n",
        "\n",
        "\n",
        "|часть речи|количество уникальных слов|общее количество слов|\n",
        "| -------- | ------------------------ | ------------------- |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "693be778",
      "metadata": {
        "id": "693be778"
      },
      "outputs": [],
      "source": [
        "def get_pos_distribution(dictionary):\n",
        "    \"\"\"\n",
        "    Создает частотное распределение частей речи в словаре.\n",
        "\n",
        "    Args:\n",
        "        dictionary (dict): Словарь от get_dictionary()\n",
        "\n",
        "    Returns:\n",
        "        dict: Словарь, где ключ - часть речи, значение - словарь с:\n",
        "            - 'unique_words' (int): количество уникальных слов\n",
        "            - 'total_words' (int): общее количество слов\n",
        "    \"\"\"\n",
        "    pos_distribution = {}\n",
        "\n",
        "    for lemma, info in dictionary.items():\n",
        "        pos = info['pos']\n",
        "        frequency = info['frequency']\n",
        "\n",
        "        if pos not in pos_distribution:\n",
        "            pos_distribution[pos] = {\n",
        "                'unique_words': 1,\n",
        "                'total_words': frequency\n",
        "            }\n",
        "        else:\n",
        "            pos_distribution[pos]['unique_words'] += 1\n",
        "            pos_distribution[pos]['total_words'] += frequency\n",
        "\n",
        "    return pos_distribution\n",
        "\n",
        "result_pos = get_pos_distribution(result_dict)\n",
        "\n",
        "# Вывод результатов\n",
        "print(\"Частотное распределение частей речи:\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"{'Часть речи':<15} {'Уникальные слова':<20} {'Всего слов':<15}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for pos, stats in result_pos.items():\n",
        "    print(f\"{pos:<15} {stats['unique_words']:<20} {stats['total_words']:<15}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa8e4f01",
      "metadata": {
        "id": "aa8e4f01"
      },
      "source": [
        "**Задание 11.** Проведите эксперименты с разработанными функциями:\n",
        "- скачайте 10 файлов с текстами разных жанров и разного размера (например, произведения классиков, современных писателей, новостные статьи, научные статьи и т.п.). *Учитывайте кодировку* – все файлы должны быть в UTF-8; Например из библиотеки Максима Мошкова lib.ru\n",
        "- обработайте файлы при помощи функций `parse_text()`, `get_dictionary()` и `get_non_mystem_dict()`, и сохраните результаты в текстовых файлах при помощи функций `save_morph_results()`, `save_dictionary()` и `save_non_mystem_dict()`. Измеряйте время запуска функций! (см. следующий пункт);\n",
        "- заполните следующую таблицу:\n",
        "\n",
        "|Файл|Размер, байт|Размер текста (кол-во слов)|Размер словаря (кол-во уникальных слов)|Время работы get_dictionary(), сек.|\n",
        "|----|------------|---------------------------|---------------------------------------|-----------------------------------|\n",
        "- для самого большого словаря постройте частотное распределение слов:\n",
        "  - по оси ординат – частота,\n",
        "  - по оси абсцисс – слова, упорядоченные по убыванию частоты (по-другому, ранги слов);\n",
        "- постройте график зависимость времени морфологического анализа от размера текстового файла;\n",
        "- распределение частей речи, полученное функцией `get_pos_distribution()`, выведите на экран в виде таблицы и графика.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Путь к папке с текстами\n",
        "texts_path = \"/content/drive/MyDrive/NLP/labs/nlp_lab1/texts\"\n",
        "\n",
        "# Получаем список файлов\n",
        "files = os.listdir(texts_path)\n",
        "files = [f for f in files if f.endswith('.txt')]\n",
        "files.sort()\n",
        "\n",
        "print(f\"Найдено файлов: {len(files)}\")\n",
        "\n",
        "# Создаем таблицу для результатов\n",
        "results_table = []\n",
        "\n",
        "# Обрабатываем каждый файл\n",
        "for file_name in files:\n",
        "    file_path = os.path.join(texts_path, file_name)\n",
        "\n",
        "    print(f\"\\nОбрабатываем файл: {file_name}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Читаем файл\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "\n",
        "    # Получаем размер файла в байтах\n",
        "    file_size = os.path.getsize(file_path)\n",
        "\n",
        "    # Замеряем время для parse_text\n",
        "    start_time = time.time()\n",
        "    parsed_data = parse_text(text)\n",
        "    parse_time = time.time() - start_time\n",
        "\n",
        "    # Замеряем время для get_dictionary\n",
        "    start_time = time.time()\n",
        "    dictionary = get_dictionary(text)\n",
        "    dict_time = time.time() - start_time\n",
        "\n",
        "    # Получаем словарь слов не в mystem\n",
        "    non_dict_words = get_non_mystem_dict(parsed_data)\n",
        "\n",
        "    # Сохраняем результаты\n",
        "    base_name = os.path.splitext(file_name)[0]\n",
        "\n",
        "    save_morph_results(parsed_data, f\"{base_name}_morph.json\")\n",
        "    save_dictionary(dictionary, f\"{base_name}_dict.json\")\n",
        "    save_non_mystem_dict(non_dict_words, f\"{base_name}_non_dict.tsv\")\n",
        "\n",
        "    # Добавляем данные в таблицу\n",
        "    results_table.append({\n",
        "        'Файл': file_name,\n",
        "        'Размер, байт': file_size,\n",
        "        'Размер текста (кол-во слов)': len(parsed_data),\n",
        "        'Размер словаря (кол-во уникальных слов)': len(dictionary),\n",
        "        'Время работы get_dictionary(), сек.': round(dict_time, 4)\n",
        "    })\n",
        "\n",
        "    print(f\"Размер файла: {file_size} байт\")\n",
        "    print(f\"Количество слов: {len(parsed_data)}\")\n",
        "    print(f\"Уникальных слов: {len(dictionary)}\")\n",
        "    print(f\"Время get_dictionary(): {dict_time:.4f} сек\")\n",
        "\n",
        "# Выводим таблицу результатов\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"РЕЗУЛЬТАТЫ АНАЛИЗА ФАЙЛОВ\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "df_results = pd.DataFrame(results_table)\n",
        "print(df_results.to_string(index=False))\n",
        "\n",
        "# Находим файл с самым большим словарем\n",
        "max_dict_file = max(results_table, key=lambda x: x['Размер словаря (кол-во уникальных слов)'])\n",
        "max_file_name = max_dict_file['Файл']\n",
        "base_name = os.path.splitext(max_file_name)[0]\n",
        "\n",
        "print(f\"\\nСамый большой словарь в файле: {max_file_name}\")\n",
        "\n",
        "# Загружаем словарь самого большого файла\n",
        "with open(f\"{base_name}_dict.json\", 'r', encoding='utf-8') as f:\n",
        "    max_dict = json.load(f)\n",
        "\n",
        "# Создаем частотное распределение для графика\n",
        "word_freq_pairs = []\n",
        "for lemma, info in max_dict.items():\n",
        "    word_freq_pairs.append((lemma, info['frequency']))\n",
        "\n",
        "# Сортируем по убыванию частоты\n",
        "word_freq_pairs.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Разделяем на слова и частоты\n",
        "words = [pair[0] for pair in word_freq_pairs]\n",
        "frequencies = [pair[1] for pair in word_freq_pairs]\n",
        "\n",
        "# Берем топ-30 самых частых слов для лучшей читаемости графика\n",
        "top_words = words[:30]\n",
        "top_frequencies = frequencies[:30]\n",
        "\n",
        "# Строим график частотного распределения\n",
        "plt.figure(figsize=(15, 6))\n",
        "plt.bar(range(len(top_words)), top_frequencies)\n",
        "plt.xlabel('Слова (упорядочены по убыванию частоты)')\n",
        "plt.ylabel('Частота')\n",
        "plt.title(f'Топ-30 самых частых слов в файле {max_file_name}')\n",
        "plt.xticks(range(len(top_words)), top_words, rotation=45, ha='right')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Строим график зависимости времени анализа от размера текста\n",
        "plt.figure(figsize=(10, 6))\n",
        "sizes = [x['Размер текста (кол-во слов)'] for x in results_table]\n",
        "times = [x['Время работы get_dictionary(), сек.'] for x in results_table]\n",
        "file_names = [x['Файл'] for x in results_table]\n",
        "\n",
        "plt.scatter(sizes, times, s=100, alpha=0.7)\n",
        "plt.xlabel('Размер текста (количество слов)')\n",
        "plt.ylabel('Время анализа (секунды)')\n",
        "plt.title('Зависимость времени морфологического анализа от размера текста')\n",
        "\n",
        "# Добавляем подписи точек\n",
        "for i, (size, time_val, name) in enumerate(zip(sizes, times, file_names)):\n",
        "    plt.annotate(name, (size, time_val), xytext=(5, 5),\n",
        "                textcoords='offset points', fontsize=8)\n",
        "\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Анализ распределения частей речи для самого большого словаря\n",
        "pos_distribution = get_pos_distribution(max_dict)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(f\"РАСПРЕДЕЛЕНИЕ ЧАСТЕЙ РЕЧИ В ФАЙЛЕ {max_file_name}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Создаем таблицу для POS распределения\n",
        "pos_table = []\n",
        "for pos, stats in pos_distribution.items():\n",
        "    pos_table.append({\n",
        "        'Часть речи': pos,\n",
        "        'Уникальные слова': stats['unique_words'],\n",
        "        'Всего слов': stats['total_words']\n",
        "    })\n",
        "\n",
        "# Сортируем по убыванию количества уникальных слов\n",
        "pos_table.sort(key=lambda x: x['Уникальные слова'], reverse=True)\n",
        "\n",
        "df_pos = pd.DataFrame(pos_table)\n",
        "print(df_pos.to_string(index=False))\n",
        "\n",
        "# Строим график распределения частей речи\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# График для уникальных слов\n",
        "plt.subplot(1, 2, 1)\n",
        "unique_words = [x['Уникальные слова'] for x in pos_table]\n",
        "pos_labels = [x['Часть речи'] for x in pos_table]\n",
        "plt.bar(range(len(unique_words)), unique_words)\n",
        "plt.xlabel('Части речи')\n",
        "plt.ylabel('Количество уникальных слов')\n",
        "plt.title('Уникальные слова по частям речи')\n",
        "plt.xticks(range(len(pos_labels)), pos_labels, rotation=45, ha='right')\n",
        "\n",
        "# График для общего количества слов\n",
        "plt.subplot(1, 2, 2)\n",
        "total_words = [x['Всего слов'] for x in pos_table]\n",
        "plt.bar(range(len(total_words)), total_words, color='orange')\n",
        "plt.xlabel('Части речи')\n",
        "plt.ylabel('Общее количество слов')\n",
        "plt.title('Все слова по частям речи')\n",
        "plt.xticks(range(len(pos_labels)), pos_labels, rotation=45, ha='right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Сохраняем итоговую таблицу в файл\n",
        "df_results.to_csv('analysis_results.csv', index=False, encoding='utf-8')\n",
        "print(\"\\nРезультаты сохранены в файл: analysis_results.csv\")"
      ],
      "metadata": {
        "id": "WS-D77svUKgG"
      },
      "id": "WS-D77svUKgG",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ta_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}