{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSgPwQvpCx3D"
      },
      "source": [
        "# Лабораторная работа 3. TF-IDF взвешивание терминов для векторизации текстов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJMAWHiLCx3G"
      },
      "source": [
        "**Задание 1.** Реализуйте векторизацию текстов на основе tf-idf метода взвешивания терминов. Предусмотрите задание размера словаря терминов, используемого для взвешивания. Можно код организовать отдельным модулем. Протестируйте свою реализацию метода."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Большинству россиян подняли зарплаты для на по? 1515 аы,\"\n",
        "text2 = preprocess(text)\n",
        "text2"
      ],
      "metadata": {
        "id": "JytCDQdn2G1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Ow_UN-diSJ9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import math\n",
        "import numpy as np\n",
        "from collections import Counter, defaultdict\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Загрузка ресурсов NLTK\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "def preprocess(text):\n",
        "    \"\"\"Токенизация текста и приведение к нижнему регистру\"\"\"\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    russian_stop_words = ['и', 'в', 'на', 'с', 'по', 'для', 'я', 'ты', 'он', 'она']\n",
        "\n",
        "    # Последовательно применяем все фильтры\n",
        "    filtered_tokens = [\n",
        "        word for word in tokens\n",
        "        if (word not in russian_stop_words\n",
        "            and len(word) > 2\n",
        "            and word.isalpha())\n",
        "    ]\n",
        "\n",
        "    return filtered_tokens\n",
        "\n",
        "def build_vocabulary(documents, max_features=None):\n",
        "    \"\"\"\n",
        "    Построение словаря терминов из документов\n",
        "    documents - список документов, каждый документ - список токенов\n",
        "    max_features - максимальное количество терминов в словаре\n",
        "    \"\"\"\n",
        "    # Считаем общую частоту каждого термина во всех документах\n",
        "    term_freq = Counter()\n",
        "    for doc in documents:\n",
        "        term_freq.update(doc)  # Увеличиваем счетчик для каждого термина в документе\n",
        "\n",
        "    # Сортируем термины по убыванию частоты\n",
        "    sorted_terms = sorted(term_freq.items(), key=lambda x: (-x[1], x[0]))\n",
        "\n",
        "    # Если задано ограничение, берем только первые max_features терминов\n",
        "    if max_features and len(sorted_terms) > max_features:\n",
        "        sorted_terms = sorted_terms[:max_features]\n",
        "\n",
        "    # Создаем словарь: термин -> индекс\n",
        "    vocabulary = {}\n",
        "    for idx, (term, freq) in enumerate(sorted_terms):\n",
        "        vocabulary[term] = idx\n",
        "\n",
        "    return vocabulary\n",
        "\n",
        "def calculate_idf(documents, vocabulary):\n",
        "    \"\"\"\n",
        "    Вычисление IDF (Inverse Document Frequency) для каждого термина\n",
        "    IDF показывает, насколько редок термин во всей коллекции документов\n",
        "    \"\"\"\n",
        "    total_docs = len(documents)\n",
        "    idf_dict = {}\n",
        "\n",
        "    # Для каждого термина в словаре считаем, в скольких документах он встречается\n",
        "    for term in vocabulary:\n",
        "        doc_count = 0\n",
        "        for doc in documents:\n",
        "            if term in doc:  # Если термин есть в документе\n",
        "                doc_count += 1\n",
        "\n",
        "        # Формула IDF с сглаживанием (smoothing)\n",
        "        # log((общее_число_документов + 1) / (документов_с_термином + 1)) + 1\n",
        "        idf_dict[term] = math.log((total_docs + 1) / (doc_count + 1)) + 1\n",
        "\n",
        "    return idf_dict\n",
        "\n",
        "def calculate_tfidf(documents, vocabulary, idf_dict):\n",
        "    \"\"\"\n",
        "    Вычисление TF-IDF матрицы\n",
        "    TF (Term Frequency) - частота термина в документе\n",
        "    TF-IDF = TF * IDF\n",
        "    \"\"\"\n",
        "    n_docs = len(documents)\n",
        "    n_terms = len(vocabulary)\n",
        "\n",
        "    # Создаем пустую матрицу (все нули)\n",
        "    tfidf_matrix = np.zeros((n_docs, n_terms))\n",
        "\n",
        "    # Для каждого документа\n",
        "    for doc_idx, doc in enumerate(documents):\n",
        "        # Считаем частоту каждого термина в текущем документе\n",
        "        term_counts = Counter(doc)\n",
        "        total_terms_in_doc = len(doc)\n",
        "\n",
        "        # Для каждого термина в документе\n",
        "        for term, count in term_counts.items():\n",
        "            # Если термин есть в нашем словаре\n",
        "            if term in vocabulary:\n",
        "                term_idx = vocabulary[term]  # Получаем индекс термина\n",
        "\n",
        "                # Вычисляем TF (нормализованная частота)\n",
        "                tf = count / total_terms_in_doc\n",
        "\n",
        "                # Вычисляем TF-IDF\n",
        "                tfidf_matrix[doc_idx, term_idx] = tf * idf_dict[term]\n",
        "\n",
        "    return tfidf_matrix\n",
        "\n",
        "def my_tfidf_vectorizer(documents, max_features=None):\n",
        "    \"\"\"\n",
        "    Основная функция TF-IDF векторизации\n",
        "    Объединяет все шаги: построение словаря, расчет IDF, расчет TF-IDF\n",
        "    \"\"\"\n",
        "    # Шаг 1: Строим словарь\n",
        "    vocabulary = build_vocabulary(documents, max_features)\n",
        "\n",
        "    # Шаг 2: Вычисляем IDF\n",
        "    idf_dict = calculate_idf(documents, vocabulary)\n",
        "\n",
        "    # Шаг 3: Вычисляем TF-IDF матрицу\n",
        "    tfidf_matrix = calculate_tfidf(documents, vocabulary, idf_dict)\n",
        "\n",
        "    return tfidf_matrix, vocabulary, idf_dict\n",
        "\n",
        "# СОЗДАНИЕ ИСХОДНЫХ ДАННЫХ\n",
        "data = {\n",
        "    'document': [\n",
        "        'Этот документ первый.',\n",
        "        'Этот документ второй.',\n",
        "        'И это третий документ.',\n",
        "        'Документ четвертый отличается.'\n",
        "    ]\n",
        "}\n",
        "df = pd.DataFrame(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxyaTafhNh_I"
      },
      "outputs": [],
      "source": [
        "print(\"1. Исходные данные:\")\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WX20BvoANq1Y"
      },
      "outputs": [],
      "source": [
        "# ПРЕДВАРИТЕЛЬНАЯ ОБРАБОТКА ТЕКСТА\n",
        "df['tokens'] = df['document'].apply(preprocess)  # Получаем списки токенов\n",
        "print(\"2. После токенизации:\")\n",
        "for i, row in df.iterrows():\n",
        "    print(f\"   Документ {i+1}: {row['tokens']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDEm9yqlNtXm"
      },
      "outputs": [],
      "source": [
        "# ВЕКТОРИЗАЦИЯ С ОГРАНИЧЕНИЕМ РАЗМЕРА СЛОВАРЯ\n",
        "max_features = 5  # Ограничиваем словарь 5 терминами\n",
        "print(f\"3. Векторизация TF-IDF (словарь ограничен {max_features} терминами)\")\n",
        "\n",
        "# Применяем нашу реализацию TF-IDF\n",
        "tfidf_matrix, vocabulary, idf_dict = my_tfidf_vectorizer(df['tokens'].tolist(), max_features)\n",
        "\n",
        "# СОЗДАЕМ DATAFRAME ДЛЯ НАГЛЯДНОСТИ\n",
        "# Получаем названия терминов в правильном порядке\n",
        "feature_names = [term for term, idx in sorted(vocabulary.items(), key=lambda x: x[1])]\n",
        "\n",
        "tfidf_df = pd.DataFrame(\n",
        "    tfidf_matrix,\n",
        "    columns=feature_names\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMSPDeSyNv5m"
      },
      "outputs": [],
      "source": [
        "print(\"4. Словарь терминов:\")\n",
        "for term, idx in sorted(vocabulary.items(), key=lambda x: x[1]):\n",
        "    print(f\"   '{term}' -> индекс {idx}, IDF: {idf_dict[term]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIVWXET-iZvX"
      },
      "outputs": [],
      "source": [
        "print(\"5. TF-IDF матрица:\")\n",
        "print(tfidf_df)\n",
        "\n",
        "print(\"\\nОбъяснение матрицы:\")\n",
        "print(\"   - Каждая строка представляет один документ\")\n",
        "print(\"   - Каждый столбец представляет один термин из словаря\")\n",
        "print(\"   - Число показывает важность термина для документа (TF-IDF вес)\")\n",
        "print(\"   - Чем выше число, тем важнее термин для документа\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-oNpGqHaOFk_"
      },
      "outputs": [],
      "source": [
        "# ДЕМОНСТРАЦИЯ РАСЧЕТА ДЛЯ ПЕРВОГО ДОКУМЕНТА\n",
        "print(\"ДЕМОНСТРАЦИЯ РАСЧЕТА ДЛЯ ПЕРВОГО ДОКУМЕНТА:\")\n",
        "\n",
        "first_doc = df['tokens'].iloc[0]\n",
        "print(f\"Документ 1: {first_doc}\")\n",
        "\n",
        "# Ручной расчет TF для первого документа\n",
        "print(\"\\nРасчет TF (Term Frequency):\")\n",
        "term_counts = Counter(first_doc)\n",
        "total_terms = len(first_doc)\n",
        "for term in first_doc:\n",
        "    if term in vocabulary:\n",
        "        tf = term_counts[term] / total_terms\n",
        "        print(f\"   '{term}': встречается {term_counts[term]} раз, всего терминов {total_terms}, TF = {term_counts[term]}/{total_terms} = {tf:.4f}\")\n",
        "\n",
        "# Показываем IDF значения\n",
        "print(\"\\nIDF значения (Inverse Document Frequency):\")\n",
        "for term in first_doc:\n",
        "    if term in vocabulary:\n",
        "        print(f\"   '{term}': IDF = {idf_dict[term]:.4f}\")\n",
        "\n",
        "# Показываем итоговые TF-IDF значения\n",
        "print(\"\\nИтоговые TF-IDF значения (TF × IDF):\")\n",
        "for term in first_doc:\n",
        "    if term in vocabulary:\n",
        "        term_idx = vocabulary[term]\n",
        "        tf = term_counts[term] / total_terms\n",
        "        tfidf_value = tf * idf_dict[term]\n",
        "        print(f\"   '{term}': TF-IDF = {tf:.4f} × {idf_dict[term]:.4f} = {tfidf_value:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXxlxgntCx3H"
      },
      "source": [
        "**Задание 2.** Сделайте классификацию новостных текстов из предыдущей лабораторной работы. Векторизация tf-idf имеет два этапа: построение словаря и построение векторов, поэтому исходный датасет надо разделить на две части. Первая часть будет использоваться для формирования словаря, а вторая часть для обучения и проверки модели классификации.\n",
        "\n",
        "Основные действия:\n",
        "- предобработка (токенизация, удаление пунктуации, лематизация),\n",
        "- разбиение на выборку текстов на две части в отношении 1:1 (первую используйте для построения словаря, а вторую часть - для построения модели классификации),\n",
        "- построение словаря для tf-idf векторизации по первой части выборки (**на основе вашей реализации tf-idf**),\n",
        "- векторизация текстов второй части выборки (**на основе вашей реализации tf-idf**),\n",
        "- классификация текстов по темам на основе **логистической регрессии** (вторую часть выборки надо снова разделит две части:обучающую и валидационную),\n",
        "- оценивание качество классификации."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwHECPEUQGbY"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BVKPL_qOQKfW"
      },
      "outputs": [],
      "source": [
        "news = pd.read_csv('/content/drive/MyDrive/NLP/labs/nlp_lab2/lenta_ru_news_filtered.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Устанавливаем библиотеку Natasha\n",
        "!pip install natasha"
      ],
      "metadata": {
        "collapsed": true,
        "id": "We1C0jASVUxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9amv8eBQV0G"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import re\n",
        "from collections import Counter\n",
        "import math\n",
        "from natasha import(\n",
        "    Segmenter,\n",
        "    MorphVocab,\n",
        "    NewsEmbedding,\n",
        "    NewsMorphTagger,\n",
        "    NewsSyntaxParser,\n",
        "    Doc,\n",
        ")\n",
        "\n",
        "# Создаем объекты для обработки\n",
        "segmenter = Segmenter()\n",
        "morph_vocab = MorphVocab()\n",
        "emb = NewsEmbedding()\n",
        "morph_tagger = NewsMorphTagger(emb)\n",
        "syntax_parser = NewsSyntaxParser(emb)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Функция лемматизации с фильтрацией\n",
        "def lemmatize_text(text):\n",
        "    doc = Doc(text)\n",
        "    doc.segment(segmenter)\n",
        "    doc.tag_morph(morph_tagger)\n",
        "\n",
        "    russian_stop_words = ['и', 'в', 'на', 'с', 'по', 'для', 'я', 'ты', 'он', 'она']\n",
        "\n",
        "    lemmas = []\n",
        "    for token in doc.tokens:\n",
        "        token.lemmatize(morph_vocab)\n",
        "        lemma = token.lemma if token.lemma else token.text\n",
        "\n",
        "        # Фильтрация\n",
        "        if (lemma not in russian_stop_words\n",
        "            and len(lemma) > 2\n",
        "            and lemma.isalpha()):\n",
        "            lemmas.append(lemma)\n",
        "\n",
        "    return ' '.join(lemmas)\n",
        "\n",
        "# Функция предобработки текста\n",
        "def preprocess_text(text):\n",
        "    # Только базовое приведение к нижнему регистру\n",
        "    text = text.lower().strip()\n",
        "\n",
        "    # Вся обработка делается в lemmatize_text\n",
        "    lemmatized_text = lemmatize_text(text)\n",
        "\n",
        "    return lemmatized_text\n",
        "\n",
        "# Применяем предобработку\n",
        "print(\"Предобработка текстов...\")\n",
        "news['processed_text'] = news['text'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "6F4GDDFOVIYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Большинству россиян подняли зарплаты для на по? 1515 аы,\"\n",
        "text1 = preprocess(text)\n",
        "text2 = preprocess_text(text)\n",
        "print(text1)\n",
        "print(text2)"
      ],
      "metadata": {
        "id": "HxKHjcej6kWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10UcS0Tm44At"
      },
      "outputs": [],
      "source": [
        "# РАЗДЕЛЕНИЕ 1: Разбиваем на две части 1:1 для словаря и классификации\n",
        "print(\"Разделение данных на две части 1:1...\")\n",
        "part1_df, part2_df = train_test_split(\n",
        "    news,\n",
        "    test_size=0.5,\n",
        "    random_state=42,\n",
        "    stratify=news['topic']\n",
        ")\n",
        "\n",
        "print(f\"Часть 1 (для словаря): {len(part1_df)} текстов\")\n",
        "print(f\"Часть 2 (для классификации): {len(part2_df)} текстов\")\n",
        "\n",
        "# Функция для токенизации текстов\n",
        "def tokenize_texts(texts):\n",
        "    \"\"\"Преобразование текстов в список токенов\"\"\"\n",
        "    tokenized_texts = []\n",
        "    for text in texts:\n",
        "        tokens = text.split()\n",
        "        tokenized_texts.append(tokens)\n",
        "    return tokenized_texts\n",
        "\n",
        "# ШАГ 1: Построение словаря по первой части\n",
        "print(\"\\nШАГ 1: Построение словаря по первой части данных...\")\n",
        "part1_tokenized = tokenize_texts(part1_df['processed_text'].tolist())\n",
        "\n",
        "# Строим словарь с ограничением в 5000 терминов\n",
        "max_features = 5000\n",
        "tfidf_matrix_part1, vocabulary, idf_dict = my_tfidf_vectorizer(part1_tokenized, max_features)\n",
        "\n",
        "print(f\"Размер словаря: {len(vocabulary)} терминов\")\n",
        "\n",
        "# ШАГ 2: Векторизация второй части с использованием построенного словаря\n",
        "print(\"\\nШАГ 2: Векторизация второй части данных...\")\n",
        "part2_tokenized = tokenize_texts(part2_df['processed_text'].tolist())\n",
        "\n",
        "# Векторизуем вторую часть, используя словарь из первой части\n",
        "tfidf_matrix_part2 = calculate_tfidf(part2_tokenized, vocabulary, idf_dict)\n",
        "print(f\"Размер TF-IDF матрицы для второй части: {tfidf_matrix_part2.shape}\")\n",
        "\n",
        "# ШАГ 3: Разделение второй части на обучающую и валидационную\n",
        "print(\"\\nШАГ 3: Разделение второй части на обучающую и валидационную...\")\n",
        "X = tfidf_matrix_part2\n",
        "y = part2_df['topic'].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Обучающая выборка: {X_train.shape[0]} текстов\")\n",
        "print(f\"Валидационная выборка: {X_test.shape[0]} текстов\")\n",
        "\n",
        "# ШАГ 4: Обучение логистической регрессии\n",
        "print(\"\\nШАГ 4: Обучение модели логистической регрессии...\")\n",
        "logreg = LogisticRegression(random_state=42, max_iter=1000)\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "# ШАГ 5: Оценка качества классификации\n",
        "print(\"\\nШАГ 5: Оценка качества классификации...\")\n",
        "y_pred = logreg.predict(X_test)\n",
        "\n",
        "# Метрики качества\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Точность (Accuracy): {accuracy:.4f}\")\n",
        "\n",
        "print(\"\\nОтчет по классификации:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Матрица ошибок\n",
        "print(\"Матрица ошибок:\")\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=np.unique(y_test),\n",
        "            yticklabels=np.unique(y_test))\n",
        "plt.title('Матрица ошибок классификации')\n",
        "plt.xlabel('Предсказанные метки')\n",
        "plt.ylabel('Истинные метки')\n",
        "plt.show()\n",
        "\n",
        "# Дополнительная информация\n",
        "print(\"\\nДОПОЛНИТЕЛЬНАЯ ИНФОРМАЦИЯ:\")\n",
        "print(f\"Всего текстов в исходном датасете: {len(news)}\")\n",
        "print(f\"Темы: {news['topic'].unique()}\")\n",
        "print(f\"Размер словаря TF-IDF: {len(vocabulary)} терминов\")\n",
        "print(f\"Размерность TF-IDF матрицы для классификации: {X.shape}\")\n",
        "\n",
        "# Анализ признаков\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"АНАЛИЗ ПРИЗНАКОВ ДЛЯ КЛАССИФИКАЦИИ\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "feature_names = list(vocabulary.keys())\n",
        "coefficients = logreg.coef_[0]  # Берем первый (и единственный) набор коэффициентов\n",
        "\n",
        "print(f\"Форма коэффициентов модели: {logreg.coef_.shape}\")\n",
        "print(f\"Классы модели: {logreg.classes_}\")\n",
        "\n",
        "# Для бинарной классификации в sklearn:\n",
        "# Положительные коэффициенты -> класс с индексом 1 (Экономика)\n",
        "# Отрицательные коэффициенты -> класс с индексом 0 (Культура)\n",
        "\n",
        "# Топ признаков для Экономики (положительные коэффициенты)\n",
        "top_economy_indices = np.argsort(coefficients)[-15:][::-1]\n",
        "print(f\"\\nТоп-15 признаков для темы 'Экономика' (положительные коэффициенты):\")\n",
        "for j, idx in enumerate(top_economy_indices, 1):\n",
        "    print(f\"  {j}. {feature_names[idx]} (вес: {coefficients[idx]:.4f})\")\n",
        "\n",
        "# Топ признаков для Культуры (отрицательные коэффициенты)\n",
        "top_culture_indices = np.argsort(coefficients)[:15]\n",
        "print(f\"\\nТоп-15 признаков для темы 'Культура' (отрицательные коэффициенты):\")\n",
        "for j, idx in enumerate(top_culture_indices, 1):\n",
        "    print(f\"  {j}. {feature_names[idx]} (вес: {coefficients[idx]:.4f})\")\n",
        "\n",
        "# Самые сильные признаки по абсолютному значению\n",
        "print(f\"\\nСамые сильные признаки (по абсолютному значению):\")\n",
        "abs_indices = np.argsort(np.abs(coefficients))[-20:][::-1]\n",
        "for j, idx in enumerate(abs_indices, 1):\n",
        "    class_association = \"Экономика\" if coefficients[idx] > 0 else \"Культура\"\n",
        "    print(f\"  {j}. {feature_names[idx]} (вес: {coefficients[idx]:.4f}, тема: {class_association})\")\n",
        "\n",
        "# Визуализация распределения коэффициентов\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Гистограмма коэффициентов\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(coefficients, bins=50, alpha=0.7, color='blue')\n",
        "plt.axvline(x=0, color='red', linestyle='--', linewidth=1)\n",
        "plt.title('Распределение коэффициентов логистической регрессии')\n",
        "plt.xlabel('Значение коэффициента')\n",
        "plt.ylabel('Частота')\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "# Top-20 самых важных признаков (по абсолютному значению)\n",
        "plt.subplot(1, 2, 2)\n",
        "top_20_indices = np.argsort(np.abs(coefficients))[-20:][::-1]\n",
        "top_20_features = [feature_names[idx] for idx in top_20_indices]\n",
        "top_20_coeffs = [coefficients[idx] for idx in top_20_indices]\n",
        "\n",
        "colors = ['red' if coef < 0 else 'blue' for coef in top_20_coeffs]\n",
        "plt.barh(range(len(top_20_features)), top_20_coeffs, color=colors)\n",
        "plt.yticks(range(len(top_20_features)), top_20_features)\n",
        "plt.xlabel('Значение коэффициента')\n",
        "plt.title('Топ-20 самых важных признаков\\n(красные - Культура, синие - Экономика)')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.grid(axis='x', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Анализ распределения тем в выборках\n",
        "print(\"\\nРАСПРЕДЕЛЕНИЕ ТЕМ В ВЫБОРКАХ:\")\n",
        "print(\"Часть 1 (для словаря):\")\n",
        "print(part1_df['topic'].value_counts())\n",
        "print(\"\\nЧасть 2 (для классификации):\")\n",
        "print(part2_df['topic'].value_counts())\n",
        "print(\"\\nОбучающая выборка:\")\n",
        "print(pd.Series(y_train).value_counts())\n",
        "print(\"\\nВалидационная выборка:\")\n",
        "print(pd.Series(y_test).value_counts())\n",
        "\n",
        "# Визуализация распределения\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "# Часть 1\n",
        "part1_df['topic'].value_counts().plot(kind='bar', ax=axes[0,0], title='Часть 1 - для словаря', color=['skyblue', 'lightcoral'])\n",
        "\n",
        "# Часть 2\n",
        "part2_df['topic'].value_counts().plot(kind='bar', ax=axes[0,1], title='Часть 2 - для классификации', color=['skyblue', 'lightcoral'])\n",
        "\n",
        "# Обучающая\n",
        "pd.Series(y_train).value_counts().plot(kind='bar', ax=axes[1,0], title='Обучающая выборка', color=['skyblue', 'lightcoral'])\n",
        "\n",
        "# Валидационная\n",
        "pd.Series(y_test).value_counts().plot(kind='bar', ax=axes[1,1], title='Валидационная выборка', color=['skyblue', 'lightcoral'])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W25jEDvqCx3I"
      },
      "source": [
        "**Задание 3.** Используя модуль TfIdfVectorizer библиотеки sklearn, сделайте классификацию новостных текстов из предыдущей лабораторной работы. Предусмотрите предобработку текстов и задание ограничение словаря при взвешивании. Оцените качество классификации. Подробнее прочитать про базовую обработку и векторизацию текстов можно в <a href=\"https://github.com/Yorko/mlcourse.ai/blob/main/jupyter_russian/tutorials/vectorizers_tutorial_mvsamsonov.ipynb\">руководстве</a>.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_C4TmbjhoPw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "\n",
        "# Используем данные из задания 2\n",
        "print(\"Используем данные из задания 2...\")\n",
        "print(f\"Часть 2 (для классификации): {len(part2_df)} текстов\")\n",
        "\n",
        "# Берем тексты из part2_df для классификации\n",
        "X_texts = part2_df['processed_text'].tolist()\n",
        "y = part2_df['topic'].values\n",
        "\n",
        "# Разделяем на обучающую и тестовую выборки (как в задании 2)\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train_texts, X_test_texts, y_train, y_test = train_test_split(\n",
        "    X_texts, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Обучающая выборка: {len(X_train_texts)} текстов\")\n",
        "print(f\"Тестовая выборка: {len(X_test_texts)} текстов\")\n",
        "\n",
        "# Создание TfidfVectorizer с ограничением словаря\n",
        "print(\"\\nСоздание TfidfVectorizer с ограничением словаря...\")\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    max_features=5000,  # Ограничение размера словаря (как в задании 2)\n",
        "    min_df=2,           # Минимальная частота термина\n",
        "    max_df=0.85,        # Максимальная частота термина\n",
        "    ngram_range=(1, 1)  # Используем только униграммы для сравнения\n",
        ")\n",
        "\n",
        "# Преобразование текстов в TF-IDF матрицы\n",
        "print(\"Векторизация текстов...\")\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_texts)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test_texts)\n",
        "\n",
        "print(f\"Размерность обучающей матрицы: {X_train_tfidf.shape}\")\n",
        "print(f\"Размерность тестовой матрицы: {X_test_tfidf.shape}\")\n",
        "print(f\"Размер словаря: {len(tfidf_vectorizer.get_feature_names_out())}\")\n",
        "\n",
        "# Обучение модели логистической регрессии\n",
        "print(\"\\nОбучение модели логистической регрессии...\")\n",
        "logreg = LogisticRegression(\n",
        "    random_state=42,\n",
        "    max_iter=1000\n",
        ")\n",
        "logreg.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Предсказание и оценка качества\n",
        "print(\"Оценка качества классификации...\")\n",
        "y_pred = logreg.predict(X_test_tfidf)\n",
        "\n",
        "# Метрики качества\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Точность (Accuracy): {accuracy:.4f}\")\n",
        "\n",
        "print(\"\\nПодробный отчет по классификации:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Матрица ошибок\n",
        "plt.figure(figsize=(8, 6))\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=np.unique(y_test),\n",
        "            yticklabels=np.unique(y_test))\n",
        "plt.title('Матрица ошибок классификации (sklearn TfidfVectorizer)')\n",
        "plt.xlabel('Предсказанные метки')\n",
        "plt.ylabel('Истинные метки')\n",
        "plt.show()\n",
        "\n",
        "# Анализ наиболее важных признаков\n",
        "print(\"\\nАнализ наиболее важных признаков...\")\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "coefficients = logreg.coef_[0]  # Для бинарной классификации\n",
        "\n",
        "# Топ признаков для Экономики (положительные коэффициенты)\n",
        "top_economy_indices = np.argsort(coefficients)[-15:][::-1]\n",
        "print(f\"\\nТоп-15 признаков для темы 'Экономика' (sklearn):\")\n",
        "for j, idx in enumerate(top_economy_indices, 1):\n",
        "    print(f\"  {j}. {feature_names[idx]} (вес: {coefficients[idx]:.4f})\")\n",
        "\n",
        "# Топ признаков для Культуры (отрицательные коэффициенты)\n",
        "top_culture_indices = np.argsort(coefficients)[:15]\n",
        "print(f\"\\nТоп-15 признаков для темы 'Культура' (sklearn):\")\n",
        "for j, idx in enumerate(top_culture_indices, 1):\n",
        "    print(f\"  {j}. {feature_names[idx]} (вес: {coefficients[idx]:.4f})\")\n",
        "\n",
        "# Сравнение с нашей реализацией из задания 2\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"СРАВНЕНИЕ РЕЗУЛЬТАТОВ\")\n",
        "print(\"=\"*60)\n",
        "print(\"Собственная реализация TF-IDF (Задание 2):\")\n",
        "print(\"- Точность: 0.9750\")\n",
        "print(\"- Размер словаря: 5000\")\n",
        "print(f\"\\nSklearn TfidfVectorizer (Задание 3):\")\n",
        "print(f\"- Точность: {accuracy:.4f}\")\n",
        "print(f\"- Размер словаря: {len(tfidf_vectorizer.get_feature_names_out())}\")\n",
        "\n",
        "# Визуализация сравнения\n",
        "methods = ['Наша реализация', 'Sklearn']\n",
        "scores = [0.9750, accuracy]\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "bars = plt.bar(methods, scores, color=['skyblue', 'lightcoral'])\n",
        "plt.title('Сравнение точности классификации')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim(0.9, 1.0)\n",
        "\n",
        "# Добавляем значения на столбцы\n",
        "for bar, score in zip(bars, scores):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
        "             f'{score:.4f}', ha='center', va='bottom')\n",
        "\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXpA1BOEEGAM"
      },
      "source": [
        "**Задание 4.** Постройте график зависимости значения `accuracy` от размера словаря, по которому выполняется tf-idf векторизация. Для этого используйте первую и вторую выборки исходного датасета, которые были получены при первом разбиении в предыдущем задании. По первой части текстов один раз строите словарь `D_common`, потом из него формируете словарь `D_k` размером `k`. По словарю `D_k` векторизуете вторую часть текстов, затем по которой обучаете и проверяете качество модели. Значение `k` меняется от [500, 1000, 1500, 2000, 3000, 5000]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIZLoyUG_QI4"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Размеры словарей для исследования\n",
        "k_list = [500, 1000, 1500, 2000, 3000, 5000]\n",
        "accuracies = []\n",
        "\n",
        "print(\"Исследуем зависимость точности от размера словаря...\")\n",
        "\n",
        "for k in k_list:\n",
        "    print(f\"\\nРазмер словаря: {k}\")\n",
        "\n",
        "    # Создаем TfidfVectorizer с текущим размером словаря\n",
        "    tfidf_vectorizer_k = TfidfVectorizer(\n",
        "        max_features=k,\n",
        "        min_df=2,\n",
        "        max_df=0.85,\n",
        "        ngram_range=(1, 1)\n",
        "    )\n",
        "\n",
        "    # Векторизация обучающих данных\n",
        "    X_train_tfidf_k = tfidf_vectorizer_k.fit_transform(X_train_texts)\n",
        "\n",
        "    # Векторизация тестовых данных\n",
        "    X_test_tfidf_k = tfidf_vectorizer_k.transform(X_test_texts)\n",
        "\n",
        "    print(f\"  Размерность матрицы: {X_train_tfidf_k.shape}\")\n",
        "\n",
        "    # Обучение модели\n",
        "    logreg_k = LogisticRegression(random_state=42, max_iter=1000)\n",
        "    logreg_k.fit(X_train_tfidf_k, y_train)\n",
        "\n",
        "    # Предсказание и оценка\n",
        "    y_pred_k = logreg_k.predict(X_test_tfidf_k)\n",
        "    accuracy_k = accuracy_score(y_test, y_pred_k)\n",
        "    accuracies.append(accuracy_k)\n",
        "\n",
        "    print(f\"  Точность: {accuracy_k:.4f}\")\n",
        "\n",
        "# Создаем DataFrame с результатами\n",
        "results_df = pd.DataFrame({\n",
        "    'Размер словаря': k_list,\n",
        "    'Точность': accuracies\n",
        "})\n",
        "\n",
        "print(\"\\nРезультаты исследования:\")\n",
        "print(results_df)\n",
        "\n",
        "# Построение графика\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(k_list, accuracies, 'o-', linewidth=2, markersize=8, color='blue')\n",
        "plt.title('Зависимость точности классификации от размера словаря TF-IDF', fontsize=14)\n",
        "plt.xlabel('Размер словаря', fontsize=12)\n",
        "plt.ylabel('Точность (Accuracy)', fontsize=12)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.xticks(k_list)\n",
        "\n",
        "# Добавляем аннотации с значениями точности\n",
        "for i, (k, acc) in enumerate(zip(k_list, accuracies)):\n",
        "    plt.annotate(f'{acc:.4f}', (k, acc), textcoords=\"offset points\",\n",
        "                 xytext=(0,10), ha='center', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Анализ результатов\n",
        "print(\"\\nАНАЛИЗ РЕЗУЛЬТАТОВ:\")\n",
        "print(\"=\"*40)\n",
        "max_acc_index = np.argmax(accuracies)\n",
        "min_acc_index = np.argmin(accuracies)\n",
        "\n",
        "print(f\"Лучшая точность: {accuracies[max_acc_index]:.4f} при размере словаря {k_list[max_acc_index]}\")\n",
        "print(f\"Худшая точность: {accuracies[min_acc_index]:.4f} при размере словаря {k_list[min_acc_index]}\")\n",
        "\n",
        "# Анализ роста точности\n",
        "if len(accuracies) > 1:\n",
        "    improvement = accuracies[-1] - accuracies[0]\n",
        "    print(f\"Улучшение точности при увеличении словаря с {k_list[0]} до {k_list[-1]}: {improvement:.4f}\")\n",
        "\n",
        "# Визуализация в виде столбчатой диаграммы\n",
        "plt.figure(figsize=(10, 6))\n",
        "bars = plt.bar(range(len(k_list)), accuracies, color='lightblue', edgecolor='darkblue')\n",
        "plt.title('Точность классификации в зависимости от размера словаря', fontsize=14)\n",
        "plt.xlabel('Размер словаря', fontsize=12)\n",
        "plt.ylabel('Точность (Accuracy)', fontsize=12)\n",
        "plt.xticks(range(len(k_list)), k_list)\n",
        "\n",
        "# Добавляем значения на столбцы\n",
        "for bar, acc in zip(bars, accuracies):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n",
        "             f'{acc:.4f}', ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "plt.ylim(min(accuracies) - 0.01, max(accuracies) + 0.02)\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elgLZbrMCx3J"
      },
      "source": [
        "**Задание 5.** Реализуйте поиск новостных текстов по текстовому запросу, задаваемому пользователем, на основе tf-idf векторизации (можно использовать любую реализацию методов)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ls9ukk5y_tPp"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Инициализация поисковой системы\n",
        "X_all_tfidf = tfidf_vectorizer.transform(part2_df['processed_text'].tolist())\n",
        "\n",
        "print(f\"Поисковая система готова. База: {len(part2_df)} новостей\")\n",
        "\n",
        "# Функция поиска\n",
        "def search_news(query, top_n=5):\n",
        "    processed_query = preprocess_text(query)\n",
        "    query_vector = tfidf_vectorizer.transform([processed_query])\n",
        "    similarities = cosine_similarity(query_vector, X_all_tfidf).flatten()\n",
        "    top_indices = np.argsort(similarities)[-top_n:][::-1]\n",
        "\n",
        "    results = []\n",
        "    for i, idx in enumerate(top_indices):\n",
        "        if similarities[idx] > 0:\n",
        "            doc = part2_df.iloc[idx]\n",
        "            results.append({\n",
        "                'rank': i + 1,\n",
        "                'similarity': similarities[idx],\n",
        "                'title': doc['title'],\n",
        "                'topic': doc['topic'],\n",
        "                'text': doc['text'][:150] + '...',\n",
        "                'url': doc['url']\n",
        "            })\n",
        "    return results\n",
        "\n",
        "# Интерактивный поиск\n",
        "print(\"\\nИНТЕРАКТИВНЫЙ ПОИСК НОВОСТЕЙ\")\n",
        "print(\"Введите поисковый запрос (для выхода введите 'выход' или нажмите Ctrl+C):\")\n",
        "\n",
        "try:\n",
        "    while True:\n",
        "        query = input(\"\\nЗапрос: \").strip()\n",
        "\n",
        "        if query.lower() == 'выход':\n",
        "            print(\"Завершение работы поисковой системы...\")\n",
        "            break\n",
        "\n",
        "        if not query:\n",
        "            print(\"Введите непустой запрос\")\n",
        "            continue\n",
        "\n",
        "        print(f\"\\nРезультаты поиска для: '{query}'\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        results = search_news(query, top_n=5)\n",
        "\n",
        "        if not results:\n",
        "            print(\"По вашему запросу ничего не найдено.\")\n",
        "        else:\n",
        "            for result in results:\n",
        "                print(f\"\\n{result['rank']}. [{result['topic']}] {result['title']}\")\n",
        "                print(f\"   Схожесть: {result['similarity']:.4f}\")\n",
        "                print(f\"   {result['text']}\")\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n\\nЗавершение работы по команде пользователя...\")\n",
        "except Exception as e:\n",
        "    print(f\"Произошла ошибка: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "topic_vocabularies = {\n",
        "    \"Культура\": [],\n",
        "    \"Экономика\": []\n",
        "}\n",
        "\n",
        "if 'feature_names' in locals() and 'coefficients' in locals():\n",
        "    top_economy_indices = np.argsort(coefficients)[-100:][::-1]\n",
        "    economy_words = [feature_names[idx] for idx in top_economy_indices]\n",
        "\n",
        "    top_culture_indices = np.argsort(coefficients)[:100]\n",
        "    culture_words = [feature_names[idx] for idx in top_culture_indices]\n",
        "\n",
        "    topic_vocabularies[\"Экономика\"] = economy_words\n",
        "    topic_vocabularies[\"Культура\"] = culture_words\n",
        "\n",
        "    # Сохраняем в Google Drive\n",
        "    save_path = '/content/drive/MyDrive/NLP/labs/nlp_lab4/topic_vocabularies.json'\n",
        "    with open(save_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(topic_vocabularies, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"Файл успешно сохранен в Google Drive: {save_path}\")\n",
        "    print(f\"Культура: {len(culture_words)} слов\")\n",
        "    print(f\"Экономика: {len(economy_words)} слов\")\n",
        "\n",
        "    print(\"\\nПервые 10 слов для темы 'Культура':\")\n",
        "    print(culture_words[:10])\n",
        "\n",
        "    print(\"\\nПервые 10 слов для темы 'Экономика':\")\n",
        "    print(economy_words[:10])\n",
        "else:\n",
        "    print(\"Переменные feature_names или coefficients не найдены.\")"
      ],
      "metadata": {
        "id": "eYUR8yOL-40p"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ta_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}