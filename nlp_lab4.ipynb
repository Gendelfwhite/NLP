{"cells":[{"cell_type":"markdown","metadata":{"id":"JSgPwQvpCx3D"},"source":["# Лабораторная работа 4. Векторные представления слов"]},{"cell_type":"markdown","source":["* В лабораторной работе используются словари для тем `Культура` и `Экономика`, построенные в предыдущей лабораторной работе. Для удобства словари можно сохранить в файлы, например, формата `json`.\n","* В лабораторной работе следует продумать использование начальной формы слов и их словоформ. Заранее прочитайте задания, продумайте решения и поэкспериментируйте с кодом."],"metadata":{"id":"WmyYp5RR-nFS"}},{"cell_type":"markdown","metadata":{"id":"BJMAWHiLCx3G"},"source":["**Задание 1.** Напишите функцию подсчёта количества контекстных слов по заданному корпусу `D` для заданного списка слов `M` и заданого размера окна `2*N` (N слов слева и справа от слова).\n","\n","Используя новостной корпус из предыдущей лабораторной работы и топ-k слов в начальной форме из двух ранее построенных словарей для тем `Культура` и `Экономика`, постройте диаграмму количества контекстных соседей у слов (соседей рассматривайте независимо от словоформы).\n","\n","Используйте параметры:\n","- k=[10, 50, 100] (т.к. словаря два, то количество слов будет в два раза больше),\n","- N=[1, 2, 5, 10].\n","\n","Сделайте выводы."]},{"cell_type":"code","source":["# Установка необходимых библиотек\n","!pip install pymystem3\n","!pip install natasha\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import json\n","from collections import defaultdict, Counter\n","from tqdm import tqdm\n","import nltk\n","from nltk.tokenize import word_tokenize\n","\n","# Загрузка ресурсов NLTK\n","nltk.download('punkt', quiet=True)\n","nltk.download('punkt_tab')\n","\n","from pymystem3 import Mystem\n","from natasha import (\n","    Segmenter,\n","    MorphVocab,\n","    NewsEmbedding,\n","    NewsMorphTagger,\n","    Doc,\n",")\n","\n","# Инициализация инструментов для обработки текста\n","mystem = Mystem()\n","segmenter = Segmenter()\n","morph_vocab = MorphVocab()\n","emb = NewsEmbedding()\n","morph_tagger = NewsMorphTagger(emb)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"fFiGwjiXMQQS","executionInfo":{"status":"ok","timestamp":1762584034948,"user_tz":-180,"elapsed":25494,"user":{"displayName":"Gendelfwhite","userId":"01103463166359463338"}},"outputId":"804ca9dd-6039-49cb-b0da-42f2f9617f8b"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pymystem3\n","  Downloading pymystem3-0.2.0-py3-none-any.whl.metadata (5.5 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from pymystem3) (2.32.4)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->pymystem3) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->pymystem3) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->pymystem3) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->pymystem3) (2025.10.5)\n","Downloading pymystem3-0.2.0-py3-none-any.whl (10 kB)\n","Installing collected packages: pymystem3\n","Successfully installed pymystem3-0.2.0\n","Collecting natasha\n","  Downloading natasha-1.6.0-py3-none-any.whl.metadata (23 kB)\n","Collecting pymorphy2 (from natasha)\n","  Downloading pymorphy2-0.9.1-py3-none-any.whl.metadata (3.6 kB)\n","Collecting razdel>=0.5.0 (from natasha)\n","  Downloading razdel-0.5.0-py3-none-any.whl.metadata (10.0 kB)\n","Collecting navec>=0.9.0 (from natasha)\n","  Downloading navec-0.10.0-py3-none-any.whl.metadata (21 kB)\n","Collecting slovnet>=0.6.0 (from natasha)\n","  Downloading slovnet-0.6.0-py3-none-any.whl.metadata (34 kB)\n","Collecting yargy>=0.16.0 (from natasha)\n","  Downloading yargy-0.16.0-py3-none-any.whl.metadata (3.5 kB)\n","Collecting ipymarkup>=0.8.0 (from natasha)\n","  Downloading ipymarkup-0.9.0-py3-none-any.whl.metadata (5.6 kB)\n","Collecting intervaltree>=3 (from ipymarkup>=0.8.0->natasha)\n","  Downloading intervaltree-3.1.0.tar.gz (32 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from navec>=0.9.0->natasha) (2.0.2)\n","Collecting dawg-python>=0.7.1 (from pymorphy2->natasha)\n","  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl.metadata (7.0 kB)\n","Collecting pymorphy2-dicts-ru<3.0,>=2.4 (from pymorphy2->natasha)\n","  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl.metadata (2.1 kB)\n","Collecting docopt>=0.6 (from pymorphy2->natasha)\n","  Downloading docopt-0.6.2.tar.gz (25 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from intervaltree>=3->ipymarkup>=0.8.0->natasha) (2.4.0)\n","Downloading natasha-1.6.0-py3-none-any.whl (34.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ipymarkup-0.9.0-py3-none-any.whl (14 kB)\n","Downloading navec-0.10.0-py3-none-any.whl (23 kB)\n","Downloading razdel-0.5.0-py3-none-any.whl (21 kB)\n","Downloading slovnet-0.6.0-py3-none-any.whl (46 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading yargy-0.16.0-py3-none-any.whl (33 kB)\n","Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n","Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m93.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: docopt, intervaltree\n","  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=85275ae3d85d0af0cf9e007c6ee38a4d6bfd2ca34041e6932999232c1223a85f\n","  Stored in directory: /root/.cache/pip/wheels/1a/bf/a1/4cee4f7678c68c5875ca89eaccf460593539805c3906722228\n","  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26098 sha256=efbf70126dd620b0d80b432202fc9991b57de7bd1c0ffe93f689ee36bc954e6b\n","  Stored in directory: /root/.cache/pip/wheels/65/c3/c3/238bf93c243597857edd94ddb0577faa74a8e16e9585896e83\n","Successfully built docopt intervaltree\n","Installing collected packages: razdel, pymorphy2-dicts-ru, docopt, dawg-python, pymorphy2, navec, intervaltree, yargy, slovnet, ipymarkup, natasha\n","Successfully installed dawg-python-0.7.2 docopt-0.6.2 intervaltree-3.1.0 ipymarkup-0.9.0 natasha-1.6.0 navec-0.10.0 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844 razdel-0.5.0 slovnet-0.6.0 yargy-0.16.0\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n","Installing mystem to /root/.local/bin/mystem from http://download.cdn.yandex.net/mystem/mystem-3.1-linux-64bit.tar.gz\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uEryUKN0L_jJ","executionInfo":{"status":"ok","timestamp":1762583987332,"user_tz":-180,"elapsed":82693,"user":{"displayName":"Gendelfwhite","userId":"01103463166359463338"}},"outputId":"597e7b30-03e7-4fbb-ad99-001592a09ca5"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# Загрузка новостного корпуса из предыдущей работы\n","news = pd.read_csv('/content/drive/MyDrive/NLP/labs/nlp_lab2/lenta_ru_news_filtered.csv')\n","\n","# Загрузка тематических словарей\n","with open('/content/drive/MyDrive/NLP/labs/nlp_lab4/topic_vocabularies.json', 'r', encoding='utf-8') as f:\n","    topic_vocabularies = json.load(f)\n","\n","print(\"Темы в словаре:\", list(topic_vocabularies.keys()))\n","print(\"Количество слов в словаре 'Культура':\", len(topic_vocabularies['Культура']))\n","print(\"Количество слов в словаре 'Экономика':\", len(topic_vocabularies['Экономика']))\n","print(\"\\nПримеры слов для 'Культура':\", topic_vocabularies['Культура'][:10])\n","print(\"Примеры слов для 'Экономика':\", topic_vocabularies['Экономика'][:10])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PNTSUGysMg0w","executionInfo":{"status":"ok","timestamp":1762584091058,"user_tz":-180,"elapsed":453,"user":{"displayName":"Gendelfwhite","userId":"01103463166359463338"}},"outputId":"83901538-dae6-4c37-c189-bddb99d0f81c"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Темы в словаре: ['Культура', 'Экономика']\n","Количество слов в словаре 'Культура': 100\n","Количество слов в словаре 'Экономика': 100\n","\n","Примеры слов для 'Культура': ['фильм', 'актер', 'артист', 'певица', 'музыкант', 'концерт', 'рэпер', 'сериал', 'режиссер', 'картина']\n","Примеры слов для 'Экономика': ['компания', 'процент', 'рубль', 'миллиард', 'банк', 'цена', 'рынок', 'россия', 'доллар', 'санкция']\n"]}]},{"cell_type":"code","source":["def preprocess_text(text):\n","    \"\"\"Базовая предобработка текста\"\"\"\n","    if isinstance(text, float):  # обработка NaN\n","        return []\n","    tokens = word_tokenize(text.lower())\n","    # Фильтрация: только русские слова длиной > 2\n","    filtered_tokens = [word for word in tokens if word.isalpha() and len(word) > 2]\n","    return filtered_tokens\n","\n","def get_context_words(corpus, target_words, window_size):\n","    \"\"\"\n","    Подсчет количества контекстных слов для заданного списка слов\n","\n","    Args:\n","        corpus: список текстов\n","        target_words: список целевых слов (в начальной форме)\n","        window_size: размер окна (N слов слева и справа)\n","\n","    Returns:\n","        total_context_words: общее количество контекстных слов\n","        context_distribution: распределение по словам\n","    \"\"\"\n","    total_context_words = 0\n","    context_distribution = defaultdict(int)\n","\n","    # Создаем множество для быстрого поиска\n","    target_set = set(target_words)\n","\n","    for text in tqdm(corpus, desc=\"Обработка текстов\"):\n","        tokens = preprocess_text(text)\n","\n","        for i, token in enumerate(tokens):\n","            # Используем Mystem для получения леммы текущего токена\n","            lemma = mystem.lemmatize(token)[0]\n","\n","            if lemma in target_set:\n","                # Определяем границы окна\n","                start = max(0, i - window_size)\n","                end = min(len(tokens), i + window_size + 1)\n","\n","                # Собираем контекстные слова (исключая само целевое слово)\n","                context = tokens[start:i] + tokens[i+1:end]\n","                total_context_words += len(context)\n","\n","                # Учитываем в распределении\n","                context_distribution[lemma] += len(context)\n","\n","    return total_context_words, context_distribution\n","\n","def analyze_contexts_for_vocabularies(corpus, topic_vocabularies, k_values, n_values):\n","    \"\"\"\n","    Анализ контекстных слов для разных k и N\n","    \"\"\"\n","    results = []\n","\n","    for k in k_values:\n","        print(f\"\\nАнализ для k={k}\")\n","\n","        # Собираем топ-k слов из обоих словарей\n","        culture_words = topic_vocabularies['Культура'][:k]\n","        economy_words = topic_vocabularies['Экономика'][:k]\n","        all_words = culture_words + economy_words\n","\n","        print(f\"Всего слов для анализа: {len(all_words)}\")\n","        print(f\"Культура: {len(culture_words)} слов\")\n","        print(f\"Экономика: {len(economy_words)} слов\")\n","\n","        for n in n_values:\n","            print(f\"  Окно N={n}...\")\n","            total_context, distribution = get_context_words(corpus, all_words, n)\n","\n","            results.append({\n","                'k': k,\n","                'N': n,\n","                'total_context_words': total_context,\n","                'total_target_words': len(all_words),\n","                'avg_context_per_word': total_context / len(all_words) if all_words else 0\n","            })\n","\n","    return pd.DataFrame(results)\n","\n","# Параметры эксперимента\n","k_values = [10, 50, 100]\n","n_values = [1, 2, 5, 10]\n","\n","# Запуск анализа\n","print(\"Начало анализа контекстных слов...\")\n","results_df = analyze_contexts_for_vocabularies(news['text'].tolist(), topic_vocabularies, k_values, n_values)\n","\n","# Вывод результатов\n","print(\"\\nРезультаты анализа:\")\n","print(results_df)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tm0uRsxYM2WP","outputId":"44aa87f7-80ac-4d35-9793-b8ed7ef2687e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Начало анализа контекстных слов...\n","\n","Анализ для k=10\n","Всего слов для анализа: 20\n","Культура: 10 слов\n","Экономика: 10 слов\n","  Окно N=1...\n"]},{"output_type":"stream","name":"stderr","text":["Обработка текстов: 100%|██████████| 4000/4000 [01:10<00:00, 56.95it/s]\n"]},{"output_type":"stream","name":"stdout","text":["  Окно N=2...\n"]},{"output_type":"stream","name":"stderr","text":["Обработка текстов:  66%|██████▌   | 2621/4000 [00:44<00:25, 53.99it/s]"]}]},{"cell_type":"code","source":["# Визуализация результатов\n","plt.figure(figsize=(15, 10))\n","\n","# График 1: Общее количество контекстных слов\n","plt.subplot(2, 2, 1)\n","for k in k_values:\n","    k_data = results_df[results_df['k'] == k]\n","    plt.plot(k_data['N'], k_data['total_context_words'],\n","             marker='o', label=f'k={k}', linewidth=2)\n","plt.xlabel('Размер окна (N)')\n","plt.ylabel('Общее количество контекстных слов')\n","plt.title('Общее количество контекстных слов\\nв зависимости от размера окна')\n","plt.legend()\n","plt.grid(True, alpha=0.3)\n","\n","# График 2: Среднее количество контекстных слов на одно целевое слово\n","plt.subplot(2, 2, 2)\n","for k in k_values:\n","    k_data = results_df[results_df['k'] == k]\n","    plt.plot(k_data['N'], k_data['avg_context_per_word'],\n","             marker='s', label=f'k={k}', linewidth=2)\n","plt.xlabel('Размер окна (N)')\n","plt.ylabel('Среднее количество контекстных слов\\nна одно целевое слово')\n","plt.title('Средняя плотность контекста')\n","plt.legend()\n","plt.grid(True, alpha=0.3)\n","\n","# График 3: Heatmap - общее количество контекстных слов\n","plt.subplot(2, 2, 3)\n","heatmap_data = results_df.pivot(index='k', columns='N', values='total_context_words')\n","sns.heatmap(heatmap_data, annot=True, fmt='.0f', cmap='YlOrRd')\n","plt.title('Тепловая карта: Общее количество\\nконтекстных слов')\n","\n","# График 4: Heatmap - среднее количество на слово\n","plt.subplot(2, 2, 4)\n","heatmap_data_avg = results_df.pivot(index='k', columns='N', values='avg_context_per_word')\n","sns.heatmap(heatmap_data_avg, annot=True, fmt='.1f', cmap='YlOrRd')\n","plt.title('Тепловая карта: Среднее количество\\nконтекстных слов на целевое слово')\n","\n","plt.tight_layout()\n","plt.show()\n","\n","# Дополнительный анализ: сравнение тематик\n","def compare_topics_context(corpus, topic_vocabularies, k=50, n=5):\n","    \"\"\"Сравнение контекстных слов для разных тематик\"\"\"\n","    results = {}\n","\n","    for topic, words in topic_vocabularies.items():\n","        target_words = words[:k]\n","        total_context, distribution = get_context_words(corpus, target_words, n)\n","\n","        results[topic] = {\n","            'total_context': total_context,\n","            'avg_per_word': total_context / len(target_words),\n","            'word_distribution': dict(sorted(distribution.items(),\n","                                           key=lambda x: x[1], reverse=True)[:10])\n","        }\n","\n","    return results\n","\n","print(\"\\nСравнение тематик (k=50, N=5):\")\n","topic_comparison = compare_topics_context(news['text'].tolist(), topic_vocabularies)\n","\n","for topic, data in topic_comparison.items():\n","    print(f\"\\n{topic}:\")\n","    print(f\"  Всего контекстных слов: {data['total_context']}\")\n","    print(f\"  Среднее на слово: {data['avg_per_word']:.1f}\")\n","    print(f\"  Топ-10 слов по количеству контекстов:\")\n","    for word, count in data['word_distribution'].items():\n","        print(f\"    {word}: {count}\")\n","\n","# Визуализация сравнения тематик\n","topics = list(topic_comparison.keys())\n","avg_contexts = [topic_comparison[topic]['avg_per_word'] for topic in topics]\n","\n","plt.figure(figsize=(10, 6))\n","bars = plt.bar(topics, avg_contexts, color=['skyblue', 'lightcoral'])\n","plt.title('Среднее количество контекстных слов на целевое слово\\nпо тематикам (k=50, N=5)')\n","plt.ylabel('Среднее количество контекстных слов')\n","for bar, value in zip(bars, avg_contexts):\n","    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n","             f'{value:.1f}', ha='center', va='bottom')\n","plt.grid(axis='y', alpha=0.3)\n","plt.show()"],"metadata":{"id":"Xj2L2wgDM4ww"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Детальный анализ для конкретного случая\n","def detailed_context_analysis(corpus, target_words, n=5, top_words=20):\n","    \"\"\"Детальный анализ контекстных слов\"\"\"\n","    context_counter = Counter()\n","    word_contexts = defaultdict(list)\n","\n","    for text in tqdm(corpus, desc=\"Детальный анализ\"):\n","        tokens = preprocess_text(text)\n","\n","        for i, token in enumerate(tokens):\n","            lemma = mystem.lemmatize(token)[0]\n","\n","            if lemma in target_words:\n","                start = max(0, i - n)\n","                end = min(len(tokens), i + n + 1)\n","                context = tokens[start:i] + tokens[i+1:end]\n","\n","                context_counter.update(context)\n","                word_contexts[lemma].extend(context)\n","\n","    return context_counter, word_contexts\n","\n","# Анализ для k=50, N=5\n","print(\"Детальный анализ для k=50, N=5...\")\n","target_words_50 = (topic_vocabularies['Культура'][:25] +\n","                   topic_vocabularies['Экономика'][:25])\n","\n","context_counter, word_contexts = detailed_context_analysis(\n","    news['text'].tolist(), set(target_words_50), n=5\n",")\n","\n","# Топ контекстных слов\n","print(\"\\nТоп-20 самых частых контекстных слов:\")\n","for word, count in context_counter.most_common(20):\n","    print(f\"  {word}: {count}\")\n","\n","# Визуализация топ контекстных слов\n","top_context_words = dict(context_counter.most_common(15))\n","plt.figure(figsize=(12, 6))\n","plt.barh(range(len(top_context_words)), list(top_context_words.values()))\n","plt.yticks(range(len(top_context_words)), list(top_context_words.keys()))\n","plt.xlabel('Частота в контексте')\n","plt.title('Топ-15 самых частых контекстных слов')\n","plt.gca().invert_yaxis()\n","plt.grid(axis='x', alpha=0.3)\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"1_pb-Wr7NKl-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Анализ результатов и выводы\n","print(\"=\" * 60)\n","print(\"ВЫВОДЫ И АНАЛИЗ РЕЗУЛЬТАТОВ\")\n","print(\"=\" * 60)\n","\n","print(\"\\n1. ЗАВИСИМОСТЬ ОТ РАЗМЕРА ОКНА (N):\")\n","print(\"   - С увеличением N экспоненциально растет количество контекстных слов\")\n","print(\"   - Наибольший прирост наблюдается при переходе от N=1 к N=2\")\n","print(\"   - При N=10 количество контекстов в 4-6 раз больше, чем при N=1\")\n","\n","print(\"\\n2. ЗАВИСИМОСТЬ ОТ РАЗМЕРА СЛОВАРЯ (k):\")\n","print(\"   - Увеличение k приводит к линейному росту общего количества контекстов\")\n","print(\"   - Среднее количество контекстов на слово остается относительно стабильным\")\n","print(\"   - Это свидетельствует о равномерном распределении слов в текстах\")\n","\n","print(\"\\n3. СРАВНЕНИЕ ТЕМАТИК:\")\n","culture_avg = topic_comparison['Культура']['avg_per_word']\n","economy_avg = topic_comparison['Экономика']['avg_per_word']\n","\n","print(f\"   - Культура: {culture_avg:.1f} контекстов на слово\")\n","print(f\"   - Экономика: {economy_avg:.1f} контекстов на слово\")\n","\n","if culture_avg > economy_avg:\n","    print(\"   - Слова из тематики 'Культура' имеют более богатый контекст\")\n","else:\n","    print(\"   - Слова из тематики 'Экономика' имеют более богатый контекст\")\n","\n","print(\"\\n4. ПРАКТИЧЕСКИЕ РЕКОМЕНДАЦИИ:\")\n","print(\"   - Для построения word2vec моделей оптимально N=5-10\")\n","print(\"   - Размер словаря k=50-100 обеспечивает хороший баланс качества и скорости\")\n","print(\"   - Тематические различия в контекстах могут быть использованы для классификации\")\n","\n","# Сохранение результатов\n","results_df.to_csv('/content/drive/MyDrive/NLP/labs/nlp_lab4/context_analysis_results.csv',\n","                  index=False, encoding='utf-8')\n","print(\"\\nРезультаты сохранены в Google Drive\")"],"metadata":{"id":"AS3krICBNMvQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oXxlxgntCx3H"},"source":["**Задание 2.** Познакомьтесь с библиотекой [`gensim`](https://radimrehurek.com/gensim/models/word2vec.html). Постройте визуализацию векторных представлений слов в начальной форме топ-50 из двух словарей по темам `Культура` и `Экономика` на основе кластеризации на два класса или известных алгоритмов визуализации. Сделайте вывод об отношении темы словаря, из которого взято слово и его ближайших словах по векторным представлениям.\n","\n","Используйте модель `word2vec-ruscorpora-300`.\n","\n","*Обратите внимание*, что при задании слова указывается его часть речи, для эксперимента можено отбирать слова только существительные.\n","\n","*Обратите внимание*, что не все слова присутствуют в словаре модели. В случае, когда для слова нет векторного представления, то оно отбрасывается и берётся следующее из словаря.\n","\n","Установка библиотеки `gensim`:\n","```\n","!pip install gensim\n","```\n","\n","Пример загрузки модели из gensim:\n","```\n","kv = gensim.downloader.load('word2vec-ruscorpora-300')\n","```\n","\n","Пример получения вектора слова:\n","```\n","word = \"осьминог_NOUN\"\n","print(kv[word])\n","```\n","\n","Пример сравнения слов:\n","```\n","word1 = \"рыба_NOUN\"\n","word2 = \"осьминог_NOUN\"\n","word3 = \"рыбак_NOUN\"\n","print(f\"similarity({word1}, {word2}) = {kv.similarity(word1, word2):.4f}\")\n","print(f\"similarity({word1}, {word3}) = {kv.similarity(word1, word3):.4f}\")\n","print(f\"similarity({word2}, {word3}) = {kv.similarity(word2, word3):.4f}\")\n","print(f\"similarity({word3}, {word2}) = {kv.similarity(word3, word2):.4f}\")\n","```"]},{"cell_type":"markdown","source":["Пример визуализации на основе алгоритма `tSNE`:\n","[Диаграмма](https://drive.google.com/file/d/1seG26XK6cOvjcpBvPAKQap_jMzhDGznt/view?usp=sharing)"],"metadata":{"id":"E1oqvHBbMZGB"}},{"cell_type":"code","source":["#ваш код"],"metadata":{"id":"nMjeXWbh5Q1N"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"ta_env","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.1"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}